{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "    By github.com/vzki (Vadim Tschernezki).\n",
    "    Low-level implementation of a Multi Layer Perceptron (MLP) in Python.\n",
    "    Utilizes batch matrix operations (forwarding and backwarding per sample*s*, not per sample).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb_samples_tr = 5000\n",
    "nb_samples_te = 1000\n",
    "\n",
    "ds_tr = mnist.train\n",
    "xs_tr = ds_tr.images[:nb_samples_tr]\n",
    "ys_tr = ds_tr.labels[:nb_samples_tr]\n",
    "\n",
    "ds_te = mnist.test\n",
    "xs_te = ds_te.images[:nb_samples_te]\n",
    "ys_te = ds_te.labels[:nb_samples_te]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(a, derived = False):\n",
    "    \n",
    "    if derived == True:\n",
    "        \n",
    "        return sigmoid(a) * (1 - sigmoid(a))\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        return 1 / (1 + np.exp(-a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_accuracy(pred, targ):\n",
    "    \n",
    "    nb_samples = float(pred.shape[0])\n",
    "    \n",
    "    labels_targ = np.argmax(targ, axis = 1)\n",
    "    labels_pred = np.argmax(pred, axis = 1)\n",
    "    \n",
    "    return np.sum(labels_targ == labels_pred) / nb_samples\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dense:\n",
    "    \n",
    "    def __init__(self, nb_inputs, nb_neurons):\n",
    "        \"\"\" Note that a dense layer is different from an input layer.\n",
    "            An input layer does not have any activation function or similar.\n",
    "            In terms of classical architecture definition, this dense layer is the\n",
    "            \"second\" layer of a neural network. \"\"\"\n",
    "        \n",
    "        # example dimensions calculation: [3, 2] (w) x [2, 1] (x) = [3, 1] (z)\n",
    "        self._weights = np.random.randn(nb_inputs, nb_neurons) / np.sqrt(nb_inputs)\n",
    "        \n",
    "        # weights for biases (biases are equal to 1, but weights get updated)\n",
    "        self._biases  = np.zeros((1, nb_neurons))\n",
    "        \n",
    "        self._delta = None\n",
    "        \n",
    "    def forward(self, a_prev):\n",
    "        \n",
    "        self._a_prev = a_prev\n",
    "        \n",
    "        self._z = (a_prev).dot(self._weights) + self._biases\n",
    "        \n",
    "        self._a = sigmoid(self._z)\n",
    "        \n",
    "        return self._a\n",
    "    \n",
    "    def backward(self, delta_next, weights_next):\n",
    "        \n",
    "        return (delta_next).dot(weights_next.T) * sigmoid(self._z, derived = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self._layers = []\n",
    "        \n",
    "    def add(self, dense):\n",
    "        \n",
    "        self._layers.append(dense)\n",
    "        \n",
    "    def build(self):\n",
    "        \n",
    "        self._deltas = [None] * len(self._layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        output = np.array(x)\n",
    "        \n",
    "        for l in self._layers:\n",
    "            \n",
    "            output = l.forward(output)\n",
    "            \n",
    "            # print output\n",
    "            \n",
    "        return output\n",
    "    \n",
    "    def backward(self, targ, pred):\n",
    "        \n",
    "        self._layers[-1]._delta = - (targ - pred) * (sigmoid(self._layers[-1]._z, derived = True))\n",
    "        \n",
    "        l_next = self._layers[-1]\n",
    "\n",
    "        for i in reversed(range(len(self._layers[:-1]))):\n",
    "            \n",
    "            self._layers[i]._delta = self._layers[i].backward(l_next._delta, l_next._weights)\n",
    "               \n",
    "            l_next = self._layers[i]\n",
    "        \n",
    "    def update(self, learning_rate):\n",
    "        \n",
    "        for i in reversed(range(len(self._layers))):\n",
    "            \n",
    "            self._layers[i]._weights -= learning_rate * self._layers[i]._a_prev.T.dot(self._layers[i]._delta)\n",
    "            \n",
    "            self._layers[i]._biases -= learning_rate * np.sum(self._layers[i]._delta, axis = 0, keepdims = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mlp = MultiLayerPerceptron()\n",
    "\n",
    "mlp.add(Dense(784, 24))\n",
    "mlp.add(Dense(24, 12))\n",
    "mlp.add(Dense(12, 10))\n",
    "\n",
    "mlp.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels without training.\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "[7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4]\n"
     ]
    }
   ],
   "source": [
    "pred = mlp.forward(xs_te)\n",
    "\n",
    "# display first 20 predictions and labels without training\n",
    "\n",
    "print \"Predicting labels without training.\"\n",
    "\n",
    "print np.argmax(pred, axis = 1)[:20]\n",
    "print np.argmax(ys_te, axis = 1)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. Testing accurcacy: 0.379\n",
      "Epoch 2. Testing accurcacy: 0.712\n",
      "Epoch 3. Testing accurcacy: 0.811\n",
      "Epoch 4. Testing accurcacy: 0.851\n",
      "Epoch 5. Testing accurcacy: 0.867\n",
      "Epoch 6. Testing accurcacy: 0.877\n",
      "Epoch 7. Testing accurcacy: 0.887\n",
      "Epoch 8. Testing accurcacy: 0.895\n",
      "Epoch 9. Testing accurcacy: 0.897\n",
      "Epoch 10. Testing accurcacy: 0.897\n",
      "Epoch 11. Testing accurcacy: 0.895\n",
      "Epoch 12. Testing accurcacy: 0.899\n",
      "Epoch 13. Testing accurcacy: 0.899\n",
      "Epoch 14. Testing accurcacy: 0.899\n",
      "Epoch 15. Testing accurcacy: 0.899\n",
      "Epoch 16. Testing accurcacy: 0.9\n",
      "Epoch 17. Testing accurcacy: 0.903\n",
      "Epoch 18. Testing accurcacy: 0.903\n",
      "Epoch 19. Testing accurcacy: 0.907\n",
      "Epoch 20. Testing accurcacy: 0.907\n",
      "Training took 2.61773300171 seconds.\n"
     ]
    }
   ],
   "source": [
    "# batch gradient descent\n",
    "\n",
    "sz_batch   = 40\n",
    "nb_epochs  = 20\n",
    "\n",
    "time_start = time.time()\n",
    "\n",
    "for j in range(nb_epochs):\n",
    "\n",
    "    for i in range(0, nb_samples_tr, sz_batch):\n",
    "\n",
    "        pred = mlp.forward(xs_tr[i : i + sz_batch])\n",
    "\n",
    "        mlp.backward(ys_tr[i : i + sz_batch], pred)\n",
    "\n",
    "        mlp.update(learning_rate = 0.1)\n",
    "        \n",
    "    pred_te = mlp.forward(xs_te)\n",
    "    \n",
    "    print \"Epoch {}. Testing accurcacy: {}\".format(j + 1, calc_accuracy(pred_te, ys_te))\n",
    "\n",
    "time_end = time.time()\n",
    "\n",
    "print \"Training took {} seconds.\".format(time_end - time_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels after training.\n",
      "[7 2 1 0 4 1 4 9 6 9 0 6 9 0 1 5 9 7 3 4]\n",
      "[7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4]\n"
     ]
    }
   ],
   "source": [
    "pred = mlp.forward(xs_te)\n",
    "\n",
    "# display first 20 predictions and labels with training\n",
    "\n",
    "print \"Predicting labels after training.\"\n",
    "\n",
    "print np.argmax(pred, axis = 1)[:20]\n",
    "print np.argmax(ys_te, axis = 1)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for test dataset: 0.907\n"
     ]
    }
   ],
   "source": [
    "print \"Accuracy for test dataset:\", calc_accuracy(pred, ys_te)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
