{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "    By github.com/vzki (Vadim Tschernezki).\n",
    "    Low-level implementation of a Multi Layer Perceptron (MLP) in Python.\n",
    "    Utilizes batch matrix operations (forwarding and backwarding per sample*s*, not per sample).\n",
    "    \n",
    "    Use case: classifying XOR. If only one neuron is used, then XOR problem can't be solved.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_samples_tr = 500\n",
    "nb_samples_te = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 0]\n",
      " [0 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 1]] [[False]\n",
      " [ True]\n",
      " [ True]\n",
      " [False]\n",
      " [ True]\n",
      " [False]\n",
      " [False]\n",
      " [ True]\n",
      " [ True]\n",
      " [False]]\n"
     ]
    }
   ],
   "source": [
    "# XOR data set\n",
    "\n",
    "xs_tr = np.random.randint(low = 0, high = 2, size = [nb_samples_tr, 2])\n",
    "ys_tr = np.logical_xor(xs_tr[:,0], xs_tr[:,1])\n",
    "ys_tr = ys_tr.reshape((nb_samples_tr, -1))\n",
    "\n",
    "xs_te = np.random.randint(low = 0, high = 2, size = [nb_samples_te, 2])\n",
    "ys_te = np.logical_xor(xs_te[:,0], xs_te[:,1])\n",
    "ys_te = ys_te.reshape((nb_samples_te, -1))\n",
    "\n",
    "print xs_te[:10], ys_te[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(a, derived = False):\n",
    "    \n",
    "    if derived == True:\n",
    "        \n",
    "        return sigmoid(a) * (1 - sigmoid(a))\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        return 1 / (1 + np.exp(-a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_accuracy(pred, targ):\n",
    "    \n",
    "    nb_samples = float(pred.shape[0])\n",
    "    \n",
    "    labels_targ = np.argmax(targ, axis = 1)\n",
    "    labels_pred = np.argmax(pred, axis = 1)\n",
    "    \n",
    "    return np.sum(labels_targ == labels_pred) / nb_samples\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dense:\n",
    "    \n",
    "    def __init__(self, nb_inputs, nb_neurons):\n",
    "        \"\"\" Note that a dense layer is different from an input layer.\n",
    "            An input layer does not have any activation function or similar.\n",
    "            In terms of classical architecture definition, this dense layer is the\n",
    "            \"second\" layer of a neural network. \"\"\"\n",
    "        \n",
    "        # example dimensions calculation: [3, 2] (w) x [2, 1] (x) = [3, 1] (z)\n",
    "        self._weights = np.random.randn(nb_inputs, nb_neurons) / np.sqrt(nb_inputs)\n",
    "        \n",
    "        # weights for biases (biases are equal to 1, but weights get updated)\n",
    "        self._biases  = np.zeros((1, nb_neurons))\n",
    "        \n",
    "        self._delta = None\n",
    "        \n",
    "    def forward(self, a_prev):\n",
    "        \n",
    "        self._a_prev = a_prev\n",
    "        \n",
    "        self._z = (a_prev).dot(self._weights) + self._biases\n",
    "        \n",
    "        self._a = sigmoid(self._z)\n",
    "        \n",
    "        return self._a\n",
    "    \n",
    "    def backward(self, delta_next, weights_next):\n",
    "        \n",
    "        return (delta_next).dot(weights_next.T) * sigmoid(self._z, derived = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self._layers = []\n",
    "        \n",
    "    def add(self, dense):\n",
    "        \n",
    "        self._layers.append(dense)\n",
    "        \n",
    "    def build(self):\n",
    "        \n",
    "        self._deltas = [None] * len(self._layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        output = np.array(x)\n",
    "        \n",
    "        for l in self._layers:\n",
    "            \n",
    "            output = l.forward(output)\n",
    "            \n",
    "            # print output\n",
    "            \n",
    "        return output\n",
    "    \n",
    "    def backward(self, targ, pred):\n",
    "        \n",
    "        self._layers[-1]._delta = - (targ - pred) * (sigmoid(self._layers[-1]._z, derived = True))\n",
    "        \n",
    "        l_next = self._layers[-1]\n",
    "\n",
    "        for i in reversed(range(len(self._layers[:-1]))):\n",
    "            \n",
    "            self._layers[i]._delta = self._layers[i].backward(l_next._delta, l_next._weights)\n",
    "               \n",
    "            l_next = self._layers[i]\n",
    "        \n",
    "    def update(self, learning_rate):\n",
    "        \n",
    "        for i in reversed(range(len(self._layers))):\n",
    "            \n",
    "            self._layers[i]._weights -= learning_rate * self._layers[i]._a_prev.T.dot(self._layers[i]._delta)\n",
    "            \n",
    "            self._layers[i]._biases -= learning_rate * np.sum(self._layers[i]._delta, axis = 0, keepdims = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mlp = MultiLayerPerceptron()\n",
    "\n",
    "# one hidden layer with 4 neurons results in accuracy of 1\n",
    "mlp.add(Dense(2, 3))\n",
    "mlp.add(Dense(3, 1))\n",
    "\n",
    "# no hidden layer results in accuracy of about 0.5\n",
    "# mlp.add(Dense(2, 1))\n",
    "# mlp.add(Dense(1, 1))\n",
    "\n",
    "\n",
    "mlp.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels without training.\n",
      "Accuracy:  0.48\n"
     ]
    }
   ],
   "source": [
    "pred = mlp.forward(xs_tr)\n",
    "\n",
    "print \"Predicting labels without training.\"\n",
    "print \"Accuracy: \", np.sum((pred > 0.5) == ys_tr) / float(ys_tr.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.74\n",
      "Accuracy:  0.74\n",
      "Accuracy:  0.74\n",
      "Accuracy:  0.74\n",
      "Accuracy:  0.74\n",
      "Accuracy:  0.52\n",
      "Accuracy:  0.52\n",
      "Accuracy:  0.52\n",
      "Accuracy:  0.52\n",
      "Accuracy:  0.52\n",
      "Accuracy:  0.52\n",
      "Accuracy:  0.52\n",
      "Accuracy:  0.52\n",
      "Accuracy:  0.52\n",
      "Accuracy:  0.52\n",
      "Accuracy:  0.52\n",
      "Accuracy:  0.52\n",
      "Accuracy:  0.52\n",
      "Accuracy:  1.0\n",
      "Accuracy:  1.0\n",
      "Training took 0.195467948914 seconds.\n"
     ]
    }
   ],
   "source": [
    "# batch gradient descent\n",
    "\n",
    "sz_batch   = 10\n",
    "nb_epochs  = 20\n",
    "\n",
    "time_start = time.time()\n",
    "\n",
    "for j in range(nb_epochs):\n",
    "\n",
    "    for i in range(0, nb_samples_tr, sz_batch):\n",
    "\n",
    "        pred = mlp.forward(xs_tr[i : i + sz_batch])\n",
    "\n",
    "        mlp.backward(ys_tr[i : i + sz_batch], pred)\n",
    "\n",
    "        mlp.update(learning_rate = 0.1)\n",
    "        \n",
    "    pred_te = mlp.forward(xs_te)\n",
    "    \n",
    "    # print \"Epoch {}. Testing accurcacy: {}\".format(j + 1, calc_accuracy(pred_te, ys_te))\n",
    "\n",
    "    # case: xor\n",
    "    \n",
    "    print \"Accuracy: \", np.sum((pred_te > 0.5)[:] == ys_te[:]) / float(ys_te.shape[0])\n",
    "    \n",
    "time_end = time.time()\n",
    "\n",
    "print \"Training took {} seconds.\".format(time_end - time_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels after training.\n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "print \"Predicting labels after training.\"\n",
    "pred = mlp.forward(xs_te)\n",
    "print \"Accuracy: \", np.sum((pred > 0.5) == ys_te) / float(nb_samples_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:                    [0 0 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0]\n",
      "                             [0 1 0 0 1 1 0 0 1 1 0 0 0 0 1 1 1 0 1 0]\n",
      "Predicted labels:            [0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 0 0 0 0 0]\n",
      "Target labels:               [0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 0 0 0 0 0]\n",
      "Correct predictions:         [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Accuracy:                    1.0\n"
     ]
    }
   ],
   "source": [
    "print \"Features: {: >60}\".format(xs_te[:20, 0])\n",
    "print \"{: >70}\".format(xs_te[:20, 1])\n",
    "print \"Predicted labels: {: >52}\".format(((pred > 0.5)[:20, 0]).astype(np.uint0).T)\n",
    "print \"Target labels: {: >55}\".format(((ys_te > 0.5)[:20, 0]).astype(np.uint0).T)\n",
    "print \"Correct predictions:{: >50}\".format(((pred > 0.5)[:20] == ys_te[:20]).astype(np.uint0)[:,0].T)\n",
    "print \"Accuracy: {: >22}\".format(np.sum((pred > 0.5)[:20] == ys_te[:20]) / 20.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
